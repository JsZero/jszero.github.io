{"pages":[{"title":"关于","text":"一个菜鸟码农的空间🏔 沿途的风景 🐕 家里的小柴犬“卷饼” 🎶 听过的演唱会 🍲 品尝过的美食","link":"/about/index.html"}],"posts":[{"title":"DolphinScheduler源码阅读日记（三）通信机制","text":"概述DolphinScheduler的通信机制是通过Netty来实现的，在Netty上加做了一些封装和抽象 Netty简介功能介绍Netty 是一个基于 Java 的高性能网络应用框架，广泛用于开发高并发、低延迟的网络服务器和客户端。它提供了一组丰富的 API 和工具，简化了网络通信的开发过程，特别是在处理大量并发连接和数据流时。 为各种传输类型（阻塞和非阻塞socket）提供统一API 基于灵活且可扩展的事件模型，允许明确分离关注点 高度可定制的线程模型——单线程、一个或多个线程池（如 SEDA） 真正的无连接数据报socket支持（自3.1版起） 基础概念 table th:first-of-type {width: 20%;} table th:nth-of-type(2) {width: 80%;} 概念 含义 Channel Channel 是 Netty 中用于网络通信的基本抽象，表示一个到网络套接字（Socket）的连接。它类似于 Java NIO 的 Channel，但提供了更多的高级功能。Channel 可以是 TCP、UDP 或者文件传输等不同类型的连接 EventLoop EventLoop 是 Netty 中的事件处理核心。每个 EventLoop 都与一个或多个 Channel 关联，负责处理这些 Channel 上的所有 I/O 操作，包括读写事件的调度。EventLoop 使用单线程模式，可以有效地避免线程间竞争 ChannelHandler ChannelHandler 是用于处理 Channel 上的 I/O 事件的组件。Netty 通过 ChannelHandler 实现了灵活的事件处理机制。常见的 ChannelHandler 类型包括 ChannelInboundHandler 和 ChannelOutboundHandler，分别用于处理入站和出站数据 Pipeline Pipeline 是 Netty 中的一个重要概念，它是 ChannelHandler 的有序链表。所有与 Channel 相关的事件（如读、写、连接等）都会沿着 Pipeline 依次传递。开发者可以根据需要在 Pipeline 中插入不同的 ChannelHandler 来实现定制的逻辑 Bootstrap Bootstrap 是用于配置和启动客户端或服务器端 Channel 的类。它简化了网络应用程序的启动过程，开发者可以通过 Bootstrap 设置相关参数，如线程池、事件处理器等 ByteBuf ByteBuf 是 Netty 提供的用于高效处理字节数据的缓冲区。与 Java NIO 中的 ByteBuffer 不同，ByteBuf 提供了更多的操作方法，并且支持动态扩展、引用计数等特性，极大地提高了处理二进制数据的效率 Future&amp;Promise Future 和 Promise 是 Netty 中的异步编程模型，用于表示异步操作的结果。Future 表示一个尚未完成的操作结果，而 Promise 则是可以手动设置结果的 Future。这两个接口帮助开发者处理异步任务的回调逻辑 Transport Transport 是 Netty 中的底层抽象，负责实现具体的网络协议（如 TCP、UDP）的传输机制。不同的 Transport 实现可以有不同的 I/O 模型（如 NIO、Epoll 等），适用于不同的操作系统和性能需求 通信机制实现关键类在MasterServer、WorkerServer中分别都存在一个RpcServer和RpcClient，用于作为RPC的服务端和客户端 序列化与反序列化Messgae的序列化与反序列化 事件处理器如何映射到对应的Processor处理（值得借鉴）##","link":"/2024/08/23/DolphinScheduler%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E6%97%A5%E8%AE%B0%EF%BC%88%E4%B8%89%EF%BC%89%E9%80%9A%E4%BF%A1%E6%9C%BA%E5%88%B6/"},{"title":"DolphinScheduler源码阅读日记（一）开发环境搭建","text":"系统环境 环境 版本 系统 macOS 12.2.1/m1 pro JRE Zulu 8.62.0.19-CA-macos-aarch64 Maven 3.8.6 Node 18.4.0 Pnpm 7.3.0 Zookeeper 3.8.4 MySQL 8.0.28 DolphinScheduler 3.2.0 搭建项目开发环境项目下载从github下载源码从dolphinscheduler源码仓库下载源码 12git clone https://github.com/apache/dolphinschedulergit checkout 3.2.0 # 切换到3.2.0分支 Zookeeper下载二进制可执行包 下载Zookeeper 3.8.4二进制可执行包 创建文件&amp;日志目录 解压压缩包，创建data、datalog目录 123cd /Users/jiashaoqi/plugin/zookeepertar -zxvf apache-zookeeper-3.8.4-bin.tar.gzmkdir data datalog 修改配置文件将conf目录下的zoo_sample.cfg文件，复制一份，重命名为zoo.cfg，修改其中数据和日志的配置，如： 123cd ./apache-zookeeper-3.8.4-bin/confcp zoo_sample.cfg zoo.cfgvi zoo.cfg 添加环境变量12vi ~/.bash_profile # 添加内容如下source ~/.bash_profile 启动Zookeeper1zkServer.sh start # 见到如图所示即为启动成功 MySQL数据库配置初始化数据库及账号创建完新数据库dolphinscheduler后，将dolphinscheduler/dolphinscheduler-dao/src/main/resources/sql/dolphinscheduler_mysql.sql下的sql文件直接在MySQL中运行，完成数据库初始化 1234567891011121314151617181920mysql&gt; create database dolphinscheduler;Query OK, 1 row affected (0.01 sec)mysql&gt; create user 'dolphin'@'localhost' identified by 'nihplod';Query OK, 0 rows affected (0.01 sec)mysql&gt; grant all privileges on dolphinscheduler.* to 'dolphin'@'localhost';Query OK, 0 rows affected (0.00 sec)mysql&gt; flush privileges;Query OK, 0 rows affected (0.00 sec)mysql&gt; use dolphinscheduler;Database changedmysql&gt; source /Users/jiashaoqi/workspace/idea/dolphinscheduler/dolphinscheduler-dao/src/main/resources/sql/dolphinscheduler_mysql.sql;Query OK, 1 row affected (0.00 sec)...Query OK, 1 row affected (0.00 sec) 后端Maven依赖下载通过idea打开项目，下载依赖，这可能需要一段时间 注意：maven下载依赖可能会出现问题，对于下载出错的依赖或者插件，可以到本地maven仓库目录下删除对应的子目录重新下载，如在下载依赖后编译时发生了如下异常 1java: 读取/Users/jiashaoqi/plugin/maven/apache-maven-3.8.6/repo/io/fabric8/kubernetes-model-core/5.10.2/kubernetes-model-core-5.10.2.jar时出错; zip file is empty 所以手动下载依赖kubernetes-model-core 12rm -rfv /Users/jiashaoqi/plugin/maven/apache-maven-3.8.6/repo/io/fabric8/kubernetes-model-core/5.10.2mvn dependency:get -DgroupId=io.fabric8 -DartifactId=kubernetes-model-core -Dversion=5.10.2 修改数据库配置 将dolphinscheduler-bom/pom.xml文件中mysql-connector-java依赖的scope修改为compile 将如下配置文件中的mysql的datasource配置为如下内容12345dolphinscheduler-master/src/main/resources/application.yaml:151dolphinscheduler-alert/dolphinscheduler-alert-server/src/main/resources/application.yaml:93dolphinscheduler-api/src/main/resources/application.yaml:229dolphinscheduler-standalone-server/src/main/resources/application.yaml:305dolphinscheduler-tools/src/main/resources/application.yaml:49 配置名 配置值 datasource.driver-class-name com.mysql.cj.jdbc.Driver datasource.url jdbc:mysql://127.0.0.1:3306/dolphinscheduler datasource.username dolphin datasource.password nihplod 修改日志级别为以下配置增加一行内容 使日志能在命令行中显示 123dolphinscheduler-master/src/main/resources/logback-spring.xmldolphinscheduler-worker/src/main/resources/logback-spring.xmldolphinscheduler-api/src/main/resources/logback-spring.xml 12345&lt;root level=&quot;INFO&quot;&gt;+ &lt;appender-ref ref=&quot;STDOUT&quot;/&gt; &lt;appender-ref ref=&quot;APILOGFILE&quot;/&gt; &lt;appender-ref ref=&quot;SKYWALKING-LOG&quot;/&gt;&lt;/root&gt; 可以再在STDOUT这个appender的pattern上套一个%clr(...)，让日志在控制台上高亮显示，如 12345678910111213&lt;configuration scan=&quot;true&quot; scanPeriod=&quot;120 seconds&quot;&gt;+ &lt;include resource=&quot;org/springframework/boot/logging/logback/defaults.xml&quot; /&gt; &lt;!-- ... --&gt; &lt;appender name=&quot;STDOUT&quot; class=&quot;ch.qos.logback.core.ConsoleAppender&quot;&gt; &lt;encoder&gt; &lt;pattern&gt;+ %clr([%level] %date{yyyy-MM-dd HH:mm:ss.SSS Z} %logger{96}:[%line] - [WorkflowInstance-%X{workflowInstanceId:-0}][TaskInstance-%X{taskInstanceId:-0}] - %msg%n) &lt;/pattern&gt; &lt;charset&gt;UTF-8&lt;/charset&gt; &lt;/encoder&gt; &lt;/appender&gt; &lt;!-- ... --&gt;&lt;/configuration&gt; 编译源代码 运行后端服务参考DolphinScheduler 普通开发模式需要启动三个服务，包括 MasterServer，WorkerServer，ApiApplicationServer MasterServer：在 Intellij IDEA 中执行 org.apache.dolphinscheduler.server.master.MasterServer 中的 main 方法，并配置 VM Options -Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql WorkerServer：在 Intellij IDEA 中执行 org.apache.dolphinscheduler.server.worker.WorkerServer 中的 main 方法，并配置 VM Options -Dlogging.config=classpath:logback-spring.xml -Ddruid.mysql.usePingMethod=false -Dspring.profiles.active=mysql ApiApplicationServer：在 Intellij IDEA 中执行 org.apache.dolphinscheduler.api.ApiApplicationServer 中的 main 方法，并配置 VM Options -Dlogging.config=classpath:logback-spring.xml -Dspring.profiles.active=api,mysql。启动完成可以浏览 Open API 文档，地址为 http://localhost:12345/dolphinscheduler/swagger-ui/index.html 前端安装依赖，运行前端服务123cd dolphinscheduler-uipnpm installpnpm run dev 截止目前，前后端已成功运行起来，浏览器访问 http://localhost:5173 ，并使用默认账户密码 admin/dolphinscheduler123 即可完成登录 完成环境搭建搭建成功效果","link":"/2024/07/27/Dolphinscheduler%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E6%97%A5%E8%AE%B0%EF%BC%88%E4%B8%80%EF%BC%89%E5%BC%80%E5%8F%91%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"},{"title":"DolphinScheduler源码阅读日记（二）MasterServer工作流调度源码解析","text":"系统架构 MasterServer MasterServer采用分布式无中心设计理念，MasterServer主要负责 DAG 任务切分、任务提交监控，并同时监听其它MasterServer和WorkerServer的健康状态。 MasterServer服务启动时向Zookeeper注册临时节点，通过监听Zookeeper临时节点变化来进行容错处理。 MasterServer基于netty提供监听服务。 该服务内主要包含: DistributedQuartz: 分布式调度组件，主要负责定时任务的启停操作，当quartz调起任务后，Master内部会有线程池具体负责处理任务的后续操作； MasterSchedulerService: 是一个扫描线程，定时扫描数据库中的t_ds_command表，根据不同的命令类型进行不同的业务操作； WorkflowExecuteRunnable: 主要是负责DAG任务切分、任务提交监控、各种不同事件类型的逻辑处理； TaskExecuteRunnable: 主要负责任务的处理和持久化，并生成任务事件提交到工作流的事件队列； EventExecuteService: 主要负责工作流实例的事件队列的轮询； StateWheelExecuteThread: 主要负责工作流和任务超时、任务重试、任务依赖的轮询，并生成对应的工作流或任务事件提交到工作流的事件队列； FailoverExecuteThread: 主要负责Master容错和Worker容错的相关逻辑； 核心概念 table th:first-of-type {width: 25%;} table th:nth-of-type(2) {width: 75%;} 概念 含义 Process Definition 工作流定义 Process Instance 工作流实例，实际运行时会被包装为WorkflowExecuteRunnable Command 事件消息 关键流程分析 以下流程都在MasterServer中执行，不再在标题中赘述 拉取事件MasterSchedulerBootstrap是用于从MySQL中拉取事件（Command）的主要线程，在MasterServer启动时启动，通过findCommands()方法找到待执行的事件，这里的事件不仅限于开始执行工作流，还有其他类型，具体参考CommandType的定义，如下 而拉取事件流程中，如下 采用无中心节点设计，所以每个节点通过取模的方式获取当前节点应该处理的事件。节点总数和当前节点的序号是如何生成的呢？从注册中心（ZK/ETCD/JDBC）获取所有节点列表，在本地生成序号，详情参考org.apache.dolphinscheduler.service.queue.MasterPriorityQueue.ServerComparator 处理事件，创建工作流处理事件，创建工作流在，如下 而创建工作流具体分为如下几个阶段，ProcessDefinition -&gt; ProcessInstance (WorkflowInstance) -&gt; WorkflowExecuteContext -&gt; WorkflowExecuteRunnable，产生WorkflowExecuteRunnable实例即为最终需要调度的工作流 1234567891011121314// ProcessDefinition -&gt; ProcessInstance(WorkflowInstance)org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteContextFactory#createWorkflowExecuteRunnableContext:56org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteContextFactory#createWorkflowInstance:81org.apache.dolphinscheduler.service.process.ProcessServiceImpl#handleCommand:317org.apache.dolphinscheduler.service.process.ProcessServiceImpl#constructProcessInstance:768org.apache.dolphinscheduler.service.process.ProcessServiceImpl#generateNewProcessInstance:586// ProcessInstance(WorkflowInstance) -&gt; WorkflowExecuteContextorg.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnableFactory#createWorkflowExecuteRunnable:79org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteContextFactory#createWorkflowExecuteRunnableContext:67// WorkflowExecuteContext -&gt; WorkflowExecuteRunnableorg.apache.dolphinscheduler.server.master.runner.MasterSchedulerBootstrap#run:137org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnableFactory#createWorkflowExecuteRunnable:80 工作流被创建出来之后会生成一个工作流事件WorkflowEvent，放在内存的阻塞队列workflowEventQueue当中 调度工作流MasterSchedulerBootstrap线程启动后，还会再启动一个WorkflowEventLooper工作流事件处理线程，用于消费上一步放入内存的阻塞队列workflowEventQueue当中的工作流事件WorkflowEvent WorkflowEventLooper线程会使用WorkflowEventHandler处理工作流事件 WorkflowEventHandler会将通过调用WorkflowExecuteRunnable工作流的call()方法，将工作流的启动异步提交到WorkflowExecuteThreadPool线程池中执行 最终调用工作流的submitPostNode()方法，开始执行工作流的节点 解析工作流节点获取待提交的TaskNode列表submitTaskNodeList，并生成对应的TaskInstance实例TODO 解析过程比想象的复杂，需要详解分析下 将任务添加到内存中的standby优先级队列（堆）readyToSubmitTaskQueue中，并在开始提交任务 之后分发步骤比较复杂，具体拆解如下 1234567891011121314// 开始提交任务org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnable#submitStandByTask:1967// 生成任务实例（TaskExecuteRunnable）org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnable#executeTask:956// 开始分发任务实例org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnable#executeTask:993// 分发org.apache.dolphinscheduler.server.master.runner.WorkflowExecuteRunnable#tryToDispatchTaskInstance:1011// 使用TaskDispatchOperator分发org.apache.dolphinscheduler.server.master.runner.execute.DefaultTaskExecuteRunnable#dispatch:41 // 将任务添加到globalTaskDispatchWaitingQueue中org.apache.dolphinscheduler.server.master.runner.operator.TaskDispatchOperator#handle:34// GlobalTaskDispatchWaitingQueueLooper线程轮询queue，使用BaseTaskDispatcher分发任务实例给workerorg.apache.dolphinscheduler.server.master.runner.GlobalTaskDispatchWaitingQueueLooper#run:79 最后使用找到可用worker节点，通过rpc启动待执行任务","link":"/2024/07/28/DolphinScheduler%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E6%97%A5%E8%AE%B0%EF%BC%88%E4%BA%8C%EF%BC%89MasterServer%E5%B7%A5%E4%BD%9C%E6%B5%81%E8%B0%83%E5%BA%A6%E6%BA%90%E7%A0%81%E8%A7%A3%E6%9E%90/"},{"title":"Akka使用教程","text":"Akka 是一个用 Scala 编写的库，用于在 JVM 平台上简化编写具有可容错的、高可伸缩性的 Java 和 Scala 的 Actor 模型应用，其同时提供了Java 和 Scala 的开发接口。Akka 允许我们专注于满足业务需求，而不是编写初级代码。在 Akka 中，Actor 之间通信的唯一机制就是消息传递。Akka 对 Actor 模型的使用提供了一个抽象级别，使得编写正确的并发、并行和分布式系统更加容易。Actor 模型贯穿了整个 Akka 库，为我们提供了一致的理解和使用它们的方法 ActorsIntroduction to Actors待补充","link":"/2022/07/02/Akka%E4%BD%BF%E7%94%A8%E6%95%99%E7%A8%8B/"},{"title":"Flink源码编译","text":"前言学习一下Flink的执行原理，需要在本地编译源码好debug运行。 编译环境 环境 版本 系统 m1 pro macbook pro 14 JRE Zulu 8.62.0.19-CA-macos-aarch64 Flink release-1.12.7-rc1 编译过程1mvn clean package -DskipTests -Dhadoop.version=2.7.1 参考下Building Apache Flink from Source 编译过程中产生如下异常并解决 报错：安装node和npm失败1[ERROR] Failed to execute goal com.github.eirslett:frontend-maven-plugin:1.6:install-node-and-npm (install node and npm) on project flink-runtime-web_2.11: Could not download Node.js: Could not download https://nodejs.org/dist/v10.9.0/node-v10.9.0-darwin-arm64.tar.gz: nodejs.org:443 failed to respond -&gt; [Help 1] 解决方案：参考eirslett/frontend-maven-plugin issue 952，FLINK-23230提到的问题修改flink/flink-runtime-web/pom.xml文件中frontend-maven-plugin的版本为1.11.0，发现还是有443异常，后来发现是maven的settings.xml中配置的proxy走的是sock5，修改成http代理后恢复正常","link":"/2022/07/03/Flink%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/"},{"title":"MySQL实战45讲-阅读笔记","text":"SQL查询语句的执行执行流程如下具体拆解如下， 连接器 长连接是指连接成功后，如果客户端持续有请求，则一直使用同一个连接。短连接则是指每次执行完很少的几次查询就断开连接，下次查询再重新建立一个。使用长连接可能会导致MySQL占用内存快速上涨，原因是执行过程中临时使用的内存是管理在连接对象里面的，这些资源会在连接断开的时候才释放。解决方案有两个 定期断开长连接 MySQL 5.7或更新版本可以执行mysql_reset_connection来重新初始化连接资源 查询缓存 除非是静态表，否则不建议开启查询缓存。表只要有一次更新操作，表关联所有缓存都失效。MySQL 8.0及以上缓存功能已被废弃 分析器依次进行如下分析，不符合词法或语法则抛出异常 词法分析：解析关键字和字段名 语法分析：判断是否符合MySQL语法 优化器决定SQL的执行顺序 执行器表权限校验，调用查询引擎接口根据索引（如果有）查询数据 SQL更新语句的执行执行流程和查询语句的流程一样，如下 redo log和binlog除此以外，更新流程还涉及两个重要的日志模块：redo log（重做日志）和 binlog（归档日志）redo log - InnoDB引擎特有的日志，可以保证crash-safe。 - 物理日志，记录的是“在某个数据页上做了什么修改”。 - 循环写的，空间固定会用完。如下所示，write pos是当前记录的位置，一边写一边后移，checkpoint是当前要擦除的位置，也是往后推移并且循环的，擦除记录前要把记录更新到数据文件。write pos和checkpoint之间的是还空着的部分，可以用来记录新的操作。如果write pos追上checkpoint，表示redolog满了不能再执行新的更新，得停下来先擦掉一些记录写入binlog，把checkpoint推进一下。binlog - MySQL的Server层实现的，所有引擎都可以使用，用于归档。 - 逻辑日志，记录的是这个语句的原始逻辑，比如“给ID=2这一行的c字段加1”。 - 可以追加写入的。binlog文件写到一定大小后会切换到下一个，不会覆盖以前的日志。 update语句的执行流程图update语句的执行流程如下，图中浅色框表示是在InnoDB内部执行的，深色框表示是在执行器中执行的。redo log的写入拆成了两个步骤：prepare和commit，也就是”两阶段提交”。 🔥事务的隔离级别隔离性与隔离级别 table th:first-of-type {width: 25%;} table th:nth-of-type(2) {width: 75%;} 级别 含义 读未提交 一个事务还没提交时，它做的变更就能被别的事务看到 读提交 一个事务提交之后，它做的变更才会被其他事务看到 可重复读 一个事务执行过程中看到的数据，总是跟这个事务在启动时看到的数据是一致的 串行化 对于同一行记录，“写”会加“写锁”，“读”会加“读锁”。当出现读写锁冲突的时候，后访问的事务必须等前一个事务执行完成，才能继续执行 级别 结果 读未提交 则V1的值就是2。这时候事务B虽然还没有提交，但是结果已经被A看到了。因此，V2、V3也都是2 读提交 则V1是1，V2的值是2。事务B的更新在提交后才能被A看到。所以， V3的值也是2 可重复读 则V1、V2是1，V3是2。之所以V2还是1，遵循的就是这个要求：事务在执行期间看到的数据前后必须是一致的 串行化 则在事务B执行“将1改成2”的时候，会被锁住。直到事务A提交后，事务B才可以继续执行。所以从A的角度看， V1、V2值是1，V3的值是2 🔥事务隔离的实现事务隔离具体是怎么实现的，这里以“可重复读”为例展开说明一下。在MySQL中，实际上每条记录在更新的时候都会同时记录一条回滚操作。记录上的最新值，通过回滚操作，都可以得到前一个状态的值。假设一个值从1被按顺序改成了2、3、4，在回滚日志里面就会有类似下面的记录。当前值是4，但是在查询这条记录的时候，不同时刻启动的事务会有不同的read-view.如图中看到的，在视图A、B、C里面，这一个记录的值分别是1、2、4，同一条记录在系统中可以存在多个版本，就是数据库的多版本并发控制（MVCC）。对于read-view A，要得到1，就必须将当前值依次执行图中所有的回滚操作得到。回滚日志不会一直保留，在不需要的时候才删除。什么时候才不需要了呢？就是当系统里没有比这个回滚日志更早的read-view的时候。所以不建议用长事务，长事务意味着系统里面会存在很老的事务视图，因此回滚日志会占用大量存储资源，还占用锁资源，也可能拖垮整个库。","link":"/2025/02/15/MySQL%E5%AE%9E%E6%88%9845%E8%AE%B2-%E9%98%85%E8%AF%BB%E7%AC%94%E8%AE%B0/"},{"title":"pyspark分布式训练","text":"","link":"/2022/07/03/pyspark%E5%88%86%E5%B8%83%E5%BC%8F%E8%AE%AD%E7%BB%83/"},{"title":"分布式事务与CAP理论","text":"待补充","link":"/2022/07/24/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E4%B8%8ECAP%E7%90%86%E8%AE%BA/"},{"title":"创建第一个Akka应用","text":"关于本文 版本：2.6之后的版本为收费版本，2.6版本的scala版使用文档见v2.6使用文档，flink在用的也是这个版本 api类型：classic比较灵活，试用一下classic版本的api 构建应用Demo配置如下的pom文件 pom.xml >folded1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950&lt;?xml version=&quot;1.0&quot; encoding=&quot;UTF-8&quot;?&gt;&lt;project xmlns=&quot;http://maven.apache.org/POM/4.0.0&quot; xmlns:xsi=&quot;http://www.w3.org/2001/XMLSchema-instance&quot; xsi:schemaLocation=&quot;http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd&quot;&gt; &lt;modelVersion&gt;4.0.0&lt;/modelVersion&gt; &lt;groupId&gt;tech.bravoqq&lt;/groupId&gt; &lt;artifactId&gt;demo-akka&lt;/artifactId&gt; &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt; &lt;properties&gt; &lt;maven.compiler.source&gt;8&lt;/maven.compiler.source&gt; &lt;maven.compiler.target&gt;8&lt;/maven.compiler.target&gt; &lt;project.build.sourceEncoding&gt;UTF-8&lt;/project.build.sourceEncoding&gt; &lt;scala.version&gt;2.12.19&lt;/scala.version&gt; &lt;scala.base.version&gt;2.12&lt;/scala.base.version&gt; &lt;akka.version&gt;2.6.21&lt;/akka.version&gt; &lt;/properties&gt; &lt;dependencies&gt; &lt;dependency&gt; &lt;groupId&gt;com.typesafe.akka&lt;/groupId&gt; &lt;artifactId&gt;akka-actor_${scala.base.version}&lt;/artifactId&gt; &lt;version&gt;${akka.version}&lt;/version&gt; &lt;/dependency&gt; &lt;/dependencies&gt; &lt;build&gt; &lt;sourceDirectory&gt;src/main/scala&lt;/sourceDirectory&gt; &lt;testSourceDirectory&gt;src/test/scala&lt;/testSourceDirectory&gt; &lt;plugins&gt; &lt;plugin&gt; &lt;groupId&gt;net.alchim31.maven&lt;/groupId&gt; &lt;artifactId&gt;scala-maven-plugin&lt;/artifactId&gt; &lt;version&gt;4.9.2&lt;/version&gt; &lt;executions&gt; &lt;execution&gt; &lt;goals&gt; &lt;goal&gt;compile&lt;/goal&gt; &lt;goal&gt;testCompile&lt;/goal&gt; &lt;/goals&gt; &lt;/execution&gt; &lt;/executions&gt; &lt;configuration&gt; &lt;scalaVersion&gt;${scala.version}&lt;/scalaVersion&gt; &lt;/configuration&gt; &lt;/plugin&gt; &lt;/plugins&gt; &lt;/build&gt;&lt;/project&gt; 官方demo给到的是一个ping-pong的系统，如下， Application.scala12345678910111213141516171819202122232425262728293031323334353637383940414243444546import akka.actor.{Actor, ActorRef, ActorSystem, PoisonPill, Props}import language.postfixOpsimport scala.concurrent.duration._case object Pingcase object Pongclass Pinger extends Actor { var countDown = 100 def receive = { case Pong =&gt; println(s&quot;${self.path} received pong, count down $countDown&quot;) if (countDown &gt; 0) { countDown -= 1 sender() ! Ping } else { sender() ! PoisonPill self ! PoisonPill } }}class Ponger(pinger: ActorRef) extends Actor { def receive = { case Ping =&gt; println(s&quot;${self.path} received ping&quot;) pinger ! Pong }}object Application extends App { val system = ActorSystem(&quot;pingpong&quot;) val pinger = system.actorOf(Props[Pinger](), &quot;pinger&quot;) val ponger = system.actorOf(Props(classOf[Ponger], pinger), &quot;ponger&quot;) import system.dispatcher system.scheduler.scheduleOnce(500 millis) { ponger ! Ping }}","link":"/2024/08/07/%E5%88%9B%E5%BB%BA%E7%AC%AC%E4%B8%80%E4%B8%AAAkka%E5%BA%94%E7%94%A8/"}],"tags":[{"name":"分布式","slug":"分布式","link":"/tags/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"dolphinscheduler","slug":"dolphinscheduler","link":"/tags/dolphinscheduler/"},{"name":"Akka","slug":"Akka","link":"/tags/Akka/"},{"name":"CAP","slug":"CAP","link":"/tags/CAP/"}],"categories":[{"name":"调度服务","slug":"调度服务","link":"/categories/%E8%B0%83%E5%BA%A6%E6%9C%8D%E5%8A%A1/"},{"name":"分布式","slug":"分布式","link":"/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"}]}